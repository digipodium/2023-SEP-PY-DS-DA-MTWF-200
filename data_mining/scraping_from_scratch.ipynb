{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # class for parsing HTML documents\n",
    "import requests # library for making HTTP requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.ndtv.com/latest'\n",
    "page = requests.get(url)\n",
    "print(page.status_code) # 200 means everything went okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_html_code = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <title>My First Website</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Hello World</h1>\n",
    "        <p class=\"subtitle\">This is a paragraph</p>\n",
    "        <p>Here is another paragraph</p>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "demo_soup = BeautifulSoup(demo_html_code, 'lxml')\n",
    "print(demo_soup.find('h1'))\n",
    "print(demo_soup.find_all('p'))\n",
    "print(demo_soup.find('p', class_='subtitle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(demo_soup.find('h1'))\n",
    "item = demo_soup.find('h1')\n",
    "print(item.text)\n",
    "item2 = demo_soup.find_all('p')[0] # first paragraph\n",
    "print(item2)\n",
    "print(item2.text)\n",
    "print(item2.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parsing data from webpage using request object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "print(soup.find('a'))\n",
    "print(soup.find('h2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = soup.findAll('h2')\n",
    "for i in nt:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in soup.find_all('span', class_='posted-by'):\n",
    "    print(data.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extracting all news from page to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = soup.find_all('div', class_='news_Itm')\n",
    "print(f\"total news cards found: {len(cards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data= [] # empty list\n",
    "for item in cards:\n",
    "    try:title = item.find('h2').text.strip()\n",
    "    except:title = None\n",
    "    try:posted_by = item.find('span', class_='posted-by').text.strip()\n",
    "    except:posted_by = None\n",
    "    try:summary = item.find('p', class_='newsCont')\n",
    "    except:summary = None\n",
    "    try: imgurl = item.find('img').attrs.get('src')\n",
    "    except: imgurl = None\n",
    "    # print(title, posted_by, summary)\n",
    "    new_data.append({\n",
    "        'title': title,\n",
    "        'posted_by': posted_by,\n",
    "        'summary': summary,\n",
    "        'imgurl': imgurl\n",
    "    })\n",
    "import pandas as pd\n",
    "pd.DataFrame(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using dputils to extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dputils.scrape import Scraper, Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.ndtv.com/india'\n",
    "scraper = Scraper(url)\n",
    "result = scraper.get_multiple_page_data(\n",
    "    target = Tag(cls='lisingNews'),\n",
    "    items = Tag(cls='news_Itm'),\n",
    "    title = Tag('h2'),\n",
    "    posted_by = Tag('span', cls='posted-by'),\n",
    "    summary = Tag('p', cls='newsCont'),\n",
    "    imgurl = Tag('img', output='src')\n",
    ")\n",
    "df = pd.DataFrame(result)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.flipkart.com/search?q=mobiles&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "scraper = Scraper(url)\n",
    "out = scraper.get_multiple_page_data(\n",
    "    target=Tag('div', cls='_1YokD2 _3Mn1Gg'),\n",
    "    items=Tag('div', cls='_1AtVbE col-12-12'),\n",
    "    title=Tag('div', cls='_4rR01T'),\n",
    "    price=Tag('div', cls='_30jeq3 _1_WHN1'),\n",
    "    link=Tag('a', cls='_1fQZEK', output='href'),\n",
    ")\n",
    "df = pd.DataFrame(out)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extracting data from a single page with dputils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product': 'APPLE iPhone 14 (Blue, 128 GB)',\n",
       " 'price': 'â‚¹60,999',\n",
       " 'highlights': 'Highlights128 GB ROM15.49 cm (6.1 inch) Super Retina XDR Display12MP + 12MP | 12MP Front CameraA15 Bionic Chip, 6 Core Processor Processor'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url ='https://www.flipkart.com/apple-iphone-14-blue-128-gb/p/itmdb77f40da6b6d?pid=MOBGHWFHSV7GUFWA&lid=LSTMOBGHWFHSV7GUFWA3AV8J8&marketplace=FLIPKART&q=mobiles&store=tyy%2F4io&srno=s_1_1&otracker=search&otracker1=search&iid=9fa0dc8d-608b-4844-8f16-fca8797cfe2e.MOBGHWFHSV7GUFWA.SEARCH&ssid=nqp9vuwdkg0000001700216770709&qH=eb4af0bf07c16429'\n",
    "scraper2 = Scraper(url)\n",
    "scraper2.get_page_data(\n",
    "    product = Tag('span', cls='B_NuCI'),\n",
    "    price = Tag(cls='_30jeq3 _16Jk6d'),\n",
    "    highlights = Tag(cls='_2cM9lP'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
